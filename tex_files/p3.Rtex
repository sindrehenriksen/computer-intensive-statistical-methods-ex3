\section{}
\label{sec:p3}
<<echo=FALSE>>=
read_chunk("../code/problem3.R")
load("../data/p3.Rdata")
nice = function(x, n=4) format(round(x, n), nsmall=n)
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\vect{x} = x_1, \dots, x_n$ and $\vect{y} = y_1, \dots, y_n$ be independent random variables, where the $x_i\text{'s}$ have an exponential distribution with intensity $\lambda_0$ and the $y_i\text{'s}$ have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $\vect{x}$ and $\vect{y}$ directly, but that we observe
%
\begin{equation*}
    z_i = \max(x_i, y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
and
%
\begin{equation*}
    u_i = \ind(x_i \geq y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
where $\ind(A)$ is 1 if $A$ is true and 0 otherwise.

Based on the observed $(z_i, u_i), \ i = 1, \dots, n$, we will use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.

\paragraph{1.} Since $z_i$ and $u_i$ are determined by $x_i$ and $y_i$, our complete data is simply $\vect{x}$ and $\vect{y}$. The likelihood of the complete data is thus
%
\begin{equation*}
    f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = \lambda_0^n \lambda_1^n \exp \left\{-\lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \right\} \, ,
\end{equation*}
%
where we have used that the variables are independent and that the PDF of an exponential random variable $w$ with rate $\lambda$ is $\lambda e^{-\lambda w}$. This gives the log likelihood
%
\begin{equation*}
    \ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \, .
\end{equation*}
%
The expectation of the log likelihood is
%
\begin{multline*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n \E[x_i \given z_i, u_i, \lambda_0^{(t)}] \\
    - \lambda_1 \sum_{i=1}^n \E[y_i \given z_i, u_i, \lambda_1^{(t)}] \, .
\end{multline*}
%
Now
%
\begin{align*}
    E[x_i \given z_i, u_i, \lambda_0^{(t)}] &= u_i z_i + (1 - u_i) \E[x_i \given x_i < z_i, \lambda_0^{(t)}] \\
    &= u_i z_i + (1 - u_i) \int_0^{z_i} x_i \frac{f(x_i)}{F(z_i)} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \int_0^{z_i} x_i \lambda_0^{(t)} e^{ -\lambda_0^{(t)} x_i} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \left[ -z_i e^{ -\lambda_0^{(t)} z_i }  - \frac{e^{-\lambda_0^{(t)} z_i}}{\lambda_0^{(t)}} + \frac{1}{\lambda_0^{(t)}}\right] \\
    &= u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \,
\end{align*}
%
and, similarly,
%
\begin{equation*}
    E[y_i \given z_i, u_i, \lambda_1^{(t)}] = (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \, .
\end{equation*}
%
Thus
%
\begin{align*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] &= n \ln \lambda_0 + n \ln \lambda_1 \\
    & \ - \lambda_0 \sum_{i=1}^n \left( u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \right) \\
    & \ - \lambda_1 \sum_{i=1}^n \left( (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \right)\, .
\end{align*}

\paragraph{2.} To maximize $Q(\lambda_0, \lambda_1) = \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}]$ we need
%
\begin{align*}
    \pd{Q}{\lambda_0} = \frac{n}{\lambda_0} - \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] = 0 \quad \Rightarrow \lambda_0 = n \left[ \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] \right]^{-1} \, ,  \\
    \pd{Q}{\lambda_1} = \frac{n}{\lambda_1} - \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] = 0 \quad \Rightarrow \lambda_1 = n \left[ \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] \right]^{-1} \, .
\end{align*}
%
The EM algorithm maximizes $Q(\vect{\lambda} \given \vect{\lambda}^{(t)})$, where $\vect{\lambda} = (\lambda_0, \lambda_1)$ iteratively, setting $\vect{\lambda}^{(t)} = \argmax_{\vect{\lambda}} \, Q(\vect{\lambda} \given \vect{\lambda}^{(t-1)})$. This is repeated until some criterion is satisfied. We choose to say that convergence is obtained when the $l_2\text{-norm}$ of the difference $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ is smaller than some user-defined tolerance. The algorithm is implemented in the following code.

<<p3_prelim, eval=FALSE>>=
@
\vspace{-1em}
<<p3_2, eval=FALSE>>=
@

The EM algorithm is run on the data specified in the files \texttt{z.txt} and \texttt{u.txt}. Setting the tolerance to 0.01, the algorithm converges after 8 iterations, giving the estimate $\vect{\lambda} = $ (\Sexpr{nice(lambda)}). The $l_2\text{-norms}$ of the step-sizes $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ are shown in \figref{fig:p3_convergence}. The values of $\vect{\lambda}$ after each iteration are shown in \figref{fig:p3_lambda}. We see that $\lambda_0$ converges faster than $\lambda_1$. The same happens also with different starting values. This might be due to 142 of the 200 data points in \texttt{z.txt} being from $\vect{x}$ (which can be seen from the number of ones in \texttt{u.txt}).

\begin{figure}
    \centering
    \includegraphics{figures/p3_convergence.pdf}
    \caption{Convergence of EM algorithm for the data in \ref{sec:p3}. The algorithm stops when the $l_2\text{-norm}$ of the step-size $\vect{\lambda^{(t)}} - \vect{\lambda^{(t-1)}} < 0.01$., i.e. after 8 iterations.}
    \label{fig:p3_convergence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/p3_lambda.pdf}
    \caption{Values of $\lambda_0$ and $\lambda_1$ after each iteration of the EM algorithm discussed in \ref{sec:p3}.}
    \label{fig:p3_lambda}
\end{figure}


\paragraph{3.}

\begin{figure}
    \centering
    \includegraphics{figures/p3_lambda.pdf}
    \caption{}
    \label{fig:hist_lambda}
\end{figure}
\paragraph{4.}