\section{}
\label{sec:p3}
<<echo=FALSE>>=
read_chunk("../code/problem3.R")
load("../data/p3.Rdata")
nice = function(x, n=4) format(round(x, n), nsmall=n)
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\vect{x} = x_1, \dots, x_n$ and $\vect{y} = y_1, \dots, y_n$ be independent random variables, where the $x_i\text{'s}$ have an exponential distribution with intensity $\lambda_0$ and the $y_i\text{'s}$ have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $\vect{x}$ and $\vect{y}$ directly, but that we observe
%
\begin{equation*}
    z_i = \max(x_i, y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
and
%
\begin{equation*}
    u_i = \ind(x_i \geq y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
where $\ind(A)$ is 1 if $A$ is true and 0 otherwise.

Based on the observed $(z_i, u_i), \ i = 1, \dots, n$, we will use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.

\paragraph{1.} Since $z_i$ and $u_i$ are determined by $x_i$ and $y_i$, our complete data is simply $\vect{x}$ and $\vect{y}$. The likelihood of the complete data is thus
%
\begin{equation*}
    f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = \lambda_0^n \lambda_1^n \exp \left\{-\lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \right\} \, ,
\end{equation*}
%
where we have used that the variables are independent and that the PDF of an exponential random variable $w$ with rate $\lambda$ is $\lambda e^{-\lambda w}$. This gives the log likelihood
%
\begin{equation*}
    \ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \, .
\end{equation*}
%
The expectation of the log likelihood is
%
\begin{multline*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n \E[x_i \given z_i, u_i, \lambda_0^{(t)}] \\
    - \lambda_1 \sum_{i=1}^n \E[y_i \given z_i, u_i, \lambda_1^{(t)}] \, .
\end{multline*}
%
Now
%
\begin{align*}
    E[x_i \given z_i, u_i, \lambda_0^{(t)}] &= u_i z_i + (1 - u_i) \E[x_i \given x_i < z_i, \lambda_0^{(t)}] \\
    &= u_i z_i + (1 - u_i) \int_0^{z_i} x_i \frac{f(x_i)}{F(z_i)} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \int_0^{z_i} x_i \lambda_0^{(t)} e^{ -\lambda_0^{(t)} x_i} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \left[ -z_i e^{ -\lambda_0^{(t)} z_i }  - \frac{e^{-\lambda_0^{(t)} z_i}}{\lambda_0^{(t)}} + \frac{1}{\lambda_0^{(t)}}\right] \\
    &= u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \,
\end{align*}
%
and, similarly,
%
\begin{equation*}
    E[y_i \given z_i, u_i, \lambda_1^{(t)}] = (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \, .
\end{equation*}
%
Thus
%
\begin{align*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] &= n \ln \lambda_0 + n \ln \lambda_1 \\
    & \ - \lambda_0 \sum_{i=1}^n \left( u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \right) \\
    & \ - \lambda_1 \sum_{i=1}^n \left( (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \right)\, .
\end{align*}

\paragraph{2.} To maximize $Q(\lambda_0, \lambda_1) = \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}]$ we need
%
\begin{align*}
    \pd{Q}{\lambda_0} = \frac{n}{\lambda_0} - \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] = 0 \quad \Rightarrow \lambda_0 = n \left[ \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] \right]^{-1} \, ,  \\
    \pd{Q}{\lambda_1} = \frac{n}{\lambda_1} - \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] = 0 \quad \Rightarrow \lambda_1 = n \left[ \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] \right]^{-1} \, .
\end{align*}
%
The EM algorithm maximizes $Q(\vect{\lambda} \given \vect{\lambda}^{(t)})$, where $\vect{\lambda} = (\lambda_0, \lambda_1)$ iteratively, setting $\vect{\lambda}^{(t)} = \argmax_{\vect{\lambda}} \, Q(\vect{\lambda} \given \vect{\lambda}^{(t-1)})$. This is repeated until some criterion is satisfied. We choose to say that convergence is obtained when the $l_2\text{-norm}$ of the difference $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ is smaller than some user-defined tolerance. The algorithm is implemented in the following code.

<<p3_prelim, eval=FALSE>>=
@
\vspace{-1em}
<<p3_2, eval=FALSE>>=
@

The EM algorithm is run on the data specified in the files \texttt{z.txt} and \texttt{u.txt}. Setting the tolerance to 0.01, the algorithm converges after 8 iterations, giving the estimate $\vect{\lambda} = $ (\Sexpr{nice(lambda_em)}). The $l_2\text{-norms}$ of the step-sizes $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ are shown in \figref{fig:p3_convergence}. The values of $\vect{\lambda}$ after each iteration are shown in \figref{fig:p3_lambda}. We see that $\lambda_0$ converges faster than $\lambda_1$. The same happens also with different starting values. This might be due to 142 of the 200 data points in \texttt{z.txt} being from $\vect{x}$ (which can be seen from the number of ones in \texttt{u.txt}).

\begin{figure}
    \centering
    \includegraphics{figures/p3_convergence.pdf}
    \caption{Convergence of EM algorithm for the data in \ref{sec:p3}. The algorithm stops when the $l_2\text{-norm}$ of the step-size $\vect{\lambda^{(t)}} - \vect{\lambda^{(t-1)}} < 0.01$., i.e. after 8 iterations.}
    \label{fig:p3_convergence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/p3_lambda.pdf}
    \caption{Values of $\lambda_0$ and $\lambda_1$ after each iteration of the EM algorithm discussed in \ref{sec:p3}.}
    \label{fig:p3_lambda}
\end{figure}


\paragraph{3.}


<<bs3c, eval=FALSE>>=
@

<<print3c, echo=FALSE>>=
@

\begin{figure}
    \centering
    \includegraphics{figures/hist_lambda.pdf}
    \caption{}
    \label{fig:hist_lambda}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{4.}
We now wish to find an analytical formula for $f_{Z_i, \, U_i}(z_i, u_i \given \lambda_0, \lambda_1)$. Throughout the following derivation, we assumes $\lambda_0$ and $\lambda_1$ are known without stating it explicitly. We start by finding
\begin{equation*}
    \prob(Z_i \leq z_i, u_i) = u_i \prob(X_i \leq z_i, X_i > Y_i) + (1-u_i) \prob(Y_i \leq z_i, Y_i > X_i) \, .
\end{equation*}
%
Now
%
\begin{align*}
    \prob(X_i \leq z_i, X_i > Y_i) &= \int_0^{z_i} \int_y^{z_i} f_Y(y) f_X(x) \dif x \dif y \\
    &= 1 - e^{-\lambda_0 z_i} + \frac{\lambda_0}{\lambda_0 + \lambda_1} (1 - e^{-(\lambda_0 + \lambda_1) z_i}) \, .
\end{align*}
%
Similarly,
%
\begin{equation*}
    \prob(Y_i \leq z_i, Y_i > X_i) =  1 - e^{-\lambda_1 z_i} + \frac{\lambda_1}{\lambda_0 + \lambda_1} (1 - e^{-(\lambda_0 + \lambda_1) z_i}) \, .
\end{equation*}
%
Next we find
%
\begin{align*}
    f_{Z_i, \, U_i} (z_i, u_i) &= \pd{}{z} \prob(Z_i \leq z_i, u_i) \\
    &= u_i \left[ \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i}) \right]
    + (1-u_i) \left[ \lambda_1 e^{-\lambda_1 z_i} (1 - e^{-\lambda_0 z_i}) \right] \\
    &= u_i g(z_i) + (1-u_i) h(z_i) \, ,
\end{align*}
%
where
%
\begin{align*}
    g(z_i) &= \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i}) \, \\
    h(z_i) &= \lambda_1 e^{-\lambda_1 z_i} (1 - e^{-\lambda_0 z_i}) \, .
\end{align*}
Next we wish to find the maximum likelihood estimate of $\lambda_0$ and $\lambda_1$. First we need the likelhood
%
\begin{equation*}
    f_{\vect{Z}, \, \vect{U}} (\vect{z}, \vect{u}) = \prod_{i \st u_i=1} g(z_i) \prod_{i \st u_i=0} h(z_i) \, .
\end{equation*}
%
The log likelihood is
%
\begin{align*}
    \log f_{\vect{Z}, \, \vect{U}} (\vect{Z}, \vect{U}) &= \sum_{i \st u_i = 1} \log g(z_i) + \sum_{i \st u_i = 0} \log h(z_i) \\
    &= \sum_{i=1}^n u_i \log g(z_i) + (1-u_i) \log h(z_i) \, .
\end{align*}
%
Now 
%
\begin{align*}
    \log g(z_i) &= \log \lambda_0 - \lambda_0 z_i + \log (1 - e^{-\lambda_1 z_i}) \, , \quad \text{and} \\
    \log h(z_i) &= \log \lambda_1 - \lambda_1 z_i + \log (1 - e^{-\lambda_0 z_i}) \, ,
\end{align*}
%
so
%
\begin{multline}
\label{eq:p3_loglik}
    \log f_{\vect{Z}, \, \vect{U}} (\vect{Z}, \vect{U}) = k \log \lambda_0 + (n-k) \log \lambda_1 \\
    + \sum_{i=1}^n \left\{ u_i \left[ \log (1 - e^{-\lambda_1 z_i}) - \lambda_0 z_i \right] + (1-u_i) \left[ \log (1 - e^{-\lambda_0 z_i}) - \lambda_1 z_i \right] \right\} \, ,
\end{multline}
%
where $k = \sum_{i=1}^n u_i$. To find the maximum we take the derivative with respect to $\lambda_0$ and $\lambda_1$;
%
\begin{equation}
\label{eq:p3_score}
\begin{split}
    \pd{}{\lambda_0} \log f &= \frac{k}{\lambda_0} + \sum_{i=1}^n \left[ (1-u_i) \frac{z_i}{e^{\lambda_0 z_i} - 1} - u_i z_i \right] \, , \quad \text{and} \\
    \pd{}{\lambda_1} \log f &= \frac{n-k}{\lambda_1} + \sum_{i=1}^n \left[ u_i \frac{z_i}{e^{\lambda_1 z_i} - 1} - (1-u_i) z_i \right] \, .
\end{split}
\end{equation}
%
Even without the sum this equation is not easy to solve analytically, and the sum seems to make it impossible. So we find the MLE numerically. This can be done by finding $\lambda_0$ and $\lambda_1$ maximizing the log-likelihood \eqref{eq:p3_loglik} directly, or by finding the roots of \eqref{eq:p3_score}. We do the latter, since it is somewhat simpler to implement in this case; see the following code.
