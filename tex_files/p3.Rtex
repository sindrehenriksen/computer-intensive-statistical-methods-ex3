\section{}
\label{sec:p3}
<<echo=FALSE>>=
read_chunk("../code/problem3.R")
load("../data/p3.Rdata")
nice = function(x, n=4) format(round(x, n), nsmall=n)
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $\vect{x} = x_1, \dots, x_n$ and $\vect{y} = y_1, \dots, y_n$ be independent random variables, where the $x_i\text{'s}$ have an exponential distribution with intensity $\lambda_0$ and the $y_i\text{'s}$ have an exponential distribution with intensity $\lambda_1$. Assume we do not observe $\vect{x}$ and $\vect{y}$ directly, but that we observe
%
\begin{equation*}
    z_i = \max(x_i, y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
and
%
\begin{equation*}
    u_i = \ind(x_i \geq y_i) \quad \text{for} \ i = 1, \dots, n \, ,
\end{equation*}
%
where $\ind(A)$ is 1 if $A$ is true and 0 otherwise.

Based on the observed $(z_i, u_i), \ i = 1, \dots, n$, we will use the EM algorithm to find the maximum likelihood estimates for $(\lambda_0, \lambda_1)$.

\paragraph{1.} Since $z_i$ and $u_i$ are determined by $x_i$ and $y_i$, our complete data is simply $\vect{x}$ and $\vect{y}$. The likelihood of the complete data is thus
%
\begin{equation*}
    f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = \lambda_0^n \lambda_1^n \exp \left\{-\lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \right\} \, ,
\end{equation*}
%
where we have used that the variables are independent and that the PDF of an exponential random variable $w$ with rate $\lambda$ is $\lambda e^{-\lambda w}$. This gives the log likelihood
%
\begin{equation*}
    \ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \, .
\end{equation*}
%
The expectation of the log likelihood is
%
\begin{multline*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] = n \ln \lambda_0 + n \ln \lambda_1 - \lambda_0 \sum_{i=1}^n \E[x_i \given z_i, u_i, \lambda_0^{(t)}] \\
    - \lambda_1 \sum_{i=1}^n \E[y_i \given z_i, u_i, \lambda_1^{(t)}] \, .
\end{multline*}
%
Now
%
\begin{align*}
    E[x_i \given z_i, u_i, \lambda_0^{(t)}] &= u_i z_i + (1 - u_i) \E[x_i \given x_i < z_i, \lambda_0^{(t)}] \\
    &= u_i z_i + (1 - u_i) \int_0^{z_i} x_i \frac{f(x_i)}{F(z_i)} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \int_0^{z_i} x_i \lambda_0^{(t)} e^{ -\lambda_0^{(t)} x_i} \dif x_i \\
    &= u_i z_i + (1 - u_i) \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \left[ -z_i e^{ -\lambda_0^{(t)} z_i }  - \frac{e^{-\lambda_0^{(t)} z_i}}{\lambda_0^{(t)}} + \frac{1}{\lambda_0^{(t)}}\right] \\
    &= u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \,
\end{align*}
%
and, similarly,
%
\begin{equation*}
    E[y_i \given z_i, u_i, \lambda_1^{(t)}] = (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \, .
\end{equation*}
%
Thus
%
\begin{align*}
    \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] &= n \ln \lambda_0 + n \ln \lambda_1 \\
    & \ - \lambda_0 \sum_{i=1}^n \left( u_i z_i + (1 - u_i) \left[\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}\right] \right) \\
    & \ - \lambda_1 \sum_{i=1}^n \left( (1 - u_i) z_i + u_i \left[\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}\right] \right)\, .
\end{align*}

\paragraph{2.} To maximize $Q(\lambda_0, \lambda_1) = \E [\ln f(\vect{x}, \vect{y} \given \lambda_0, \lambda_1) \given \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}]$ we need
%
\begin{align*}
    \pd{Q}{\lambda_0} = \frac{n}{\lambda_0} - \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] = 0 \quad \Rightarrow \lambda_0 = n \left[ \sum_{i=1}^n E[x_i \given z_i, u_i, \lambda_0^{(t)}] \right]^{-1} \, ,  \\
    \pd{Q}{\lambda_1} = \frac{n}{\lambda_1} - \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] = 0 \quad \Rightarrow \lambda_1 = n \left[ \sum_{i=1}^n E[y_i \given z_i, u_i, \lambda_1^{(t)}] \right]^{-1} \, .
\end{align*}
%
The EM algorithm maximizes $Q(\vect{\lambda} \given \vect{\lambda}^{(t)})$, where $\vect{\lambda} = (\lambda_0, \lambda_1)$ iteratively, setting $\vect{\lambda}^{(t)} = \argmax_{\vect{\lambda}} \, Q(\vect{\lambda} \given \vect{\lambda}^{(t-1)})$. This is repeated until some criterion is satisfied. We choose to say that convergence is obtained when the $l_2\text{-norm}$ of the difference $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ is smaller than some user-defined tolerance. The algorithm is implemented in the following code.

<<p3_prelim, eval=FALSE>>=
@
\vspace{-1em}
<<p3_2, eval=FALSE>>=
@

The EM algorithm is run on the data specified in the files \texttt{z.txt} and \texttt{u.txt}. Setting the tolerance to 0.01, the algorithm converges after 8 iterations, giving the estimate $\vect{\lambda} = $ (\Sexpr{nice(lambda_em)}). The $l_2\text{-norms}$ of the step-sizes $\vect{\lambda}^{(t)} - \vect{\lambda}^{(t-1)}$ are shown in \figref{fig:p3_convergence}. The values of $\vect{\lambda}$ after each iteration are shown in \figref{fig:p3_lambda}. We see that $\lambda_0$ converges faster than $\lambda_1$. The same happens also with different starting values. This might be due to 142 of the 200 data points in \texttt{z.txt} being from $\vect{x}$ (which can be seen from the number of ones in \texttt{u.txt}).

\begin{figure}
    \centering
    \includegraphics{figures/p3_convergence.pdf}
    \caption{Convergence of EM algorithm for the data in \ref{sec:p3}. The algorithm stops when the $l_2\text{-norm}$ of the step-size $\vect{\lambda^{(t)}} - \vect{\lambda^{(t-1)}} < 0.01$., i.e. after 8 iterations.}
    \label{fig:p3_convergence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/p3_lambda.pdf}
    \caption{Values of $\lambda_0$ and $\lambda_1$ after each iteration of the EM algorithm discussed in \ref{sec:p3}.}
    \label{fig:p3_lambda}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{3.}
Similarly to what we did in problem A, we now wish to use bootstrapping to find the standard deviations and the biases of each of $\hat{\lambda}_1$ and $\hat{\lambda}_1$. The algorithm is like follows: Take bootstrap samples of the observation pairs and run the EM-algorithm on the bootstrap samples, repeat B times. Then calculate the standard deviation of the bootstrap estimators and the bias, i.e. the mean of the bootstrap samples minus the original estimator. We also find the correlation of the estimated parameters.

<<bs3c, eval=FALSE>>=
@
\vspace{-1em}
<<print3c, echo=FALSE>>=
@

\figref{fig:hist_lambda} contains a histogram of the bootstrap estimators. The standard deviations are 0.25 and 0.79 and the biases are 0.02 and 0.12, for $\hat{\lambda}_0$ and $\hat{\lambda}_1$, respectively. The correlation is 0.02. The biases are not very large; $\approx$ 1\% of the size of the original estimators. Thus we are tempted not to use the bias corrected estimators, since they have a larger variance than the original estimators. We could use just one bias corrected estimator and the original estimator for the other component since they seem to be almost uncorrelated. But the biases are both similarly small, so in this case they are probably better left uncorrected.

\begin{figure}
    \centering
    \includegraphics{figures/hist_lambda.pdf}
    \caption{Histogram of frequency of bootstrap estimators of $(\lambda_0, \lambda_1)$, found as explained in \ref{sec:p3}3.}
    \label{fig:hist_lambda}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{4.}
We now wish to find an analytical formula for $f_{Z_i, \, U_i}(z_i, u_i \given \lambda_0, \lambda_1)$. Throughout the following derivation, we assumes $\lambda_0$ and $\lambda_1$ are known without stating it explicitly. We start by finding
\begin{equation*}
    \prob(Z_i \leq z_i, u_i) = u_i \prob(X_i \leq z_i, X_i > Y_i) + (1-u_i) \prob(Y_i \leq z_i, Y_i > X_i) \, .
\end{equation*}
%
Now
%
\begin{align*}
    \prob(X_i \leq z_i, X_i > Y_i) &= \int_0^{z_i} \int_y^{z_i} f_Y(y) f_X(x) \dif x \dif y \\
    &= 1 - e^{-\lambda_0 z_i} + \frac{\lambda_0}{\lambda_0 + \lambda_1} (1 - e^{-(\lambda_0 + \lambda_1) z_i}) \, .
\end{align*}
%
Similarly,
%
\begin{equation*}
    \prob(Y_i \leq z_i, Y_i > X_i) =  1 - e^{-\lambda_1 z_i} + \frac{\lambda_1}{\lambda_0 + \lambda_1} (1 - e^{-(\lambda_0 + \lambda_1) z_i}) \, .
\end{equation*}
%
Next we find
%
\begin{align*}
    f_{Z_i, \, U_i} (z_i, u_i) &= \pd{}{z} \prob(Z_i \leq z_i, u_i) \\
    &= u_i \left[ \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i}) \right]
    + (1-u_i) \left[ \lambda_1 e^{-\lambda_1 z_i} (1 - e^{-\lambda_0 z_i}) \right] \\
    &= u_i g(z_i) + (1-u_i) h(z_i) \, ,
\end{align*}
%
where
%
\begin{align*}
    g(z_i) &= \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i}) \, \\
    h(z_i) &= \lambda_1 e^{-\lambda_1 z_i} (1 - e^{-\lambda_0 z_i}) \, .
\end{align*}
Next we wish to find the maximum likelihood estimate of $\lambda_0$ and $\lambda_1$. First we need the likelihood
%
\begin{equation*}
    f_{\vect{Z}, \, \vect{U}} (\vect{z}, \vect{u}) = \prod_{i \st u_i=1} g(z_i) \prod_{i \st u_i=0} h(z_i) \, .
\end{equation*}
%
The log likelihood is
%
\begin{align*}
    \log f_{\vect{Z}, \, \vect{U}} (\vect{Z}, \vect{U}) &= \sum_{i \st u_i = 1} \log g(z_i) + \sum_{i \st u_i = 0} \log h(z_i) \\
    &= \sum_{i=1}^n u_i \log g(z_i) + (1-u_i) \log h(z_i) \, .
\end{align*}
%
Now 
%
\begin{align*}
    \log g(z_i) &= \log \lambda_0 - \lambda_0 z_i + \log (1 - e^{-\lambda_1 z_i}) \, , \quad \text{and} \\
    \log h(z_i) &= \log \lambda_1 - \lambda_1 z_i + \log (1 - e^{-\lambda_0 z_i}) \, ,
\end{align*}
%
so
%
\begin{multline}
\label{eq:p3_loglik}
    \log f_{\vect{Z}, \, \vect{U}} (\vect{Z}, \vect{U}) = k \log \lambda_0 + (n-k) \log \lambda_1 \\
    + \sum_{i=1}^n \left\{ u_i \left[ \log (1 - e^{-\lambda_1 z_i}) - \lambda_0 z_i \right] + (1-u_i) \left[ \log (1 - e^{-\lambda_0 z_i}) - \lambda_1 z_i \right] \right\} \, ,
\end{multline}
%
where $k = \sum_{i=1}^n u_i$. To find the maximum we take the derivative with respect to $\lambda_0$ and $\lambda_1$;
%
\begin{equation}
\label{eq:p3_score}
\begin{split}
    \pd{}{\lambda_0} \log f &= \frac{k}{\lambda_0} + \sum_{i=1}^n \left[ (1-u_i) \frac{z_i}{e^{\lambda_0 z_i} - 1} - u_i z_i \right] \, , \quad \text{and} \\
    \pd{}{\lambda_1} \log f &= \frac{n-k}{\lambda_1} + \sum_{i=1}^n \left[ u_i \frac{z_i}{e^{\lambda_1 z_i} - 1} - (1-u_i) z_i \right] \, ,
\end{split}
\end{equation}
%
and we let this equal 0. Even without the sum this equation is not easy to solve analytically, and the sum seems to make it impossible. So we find the MLE numerically. This can be done by finding $\lambda_0$ and $\lambda_1$ maximizing the log-likelihood \eqref{eq:p3_loglik} directly, or by finding the roots of \eqref{eq:p3_score}. We do the latter, since it is somewhat simpler to implement in this case.

To be sure that we find a global maximum and not a local one, we prove that the Hessian is negative definite. First we notice
%
\begin{equation*}
    \pd{^2}{\lambda_0 \partial \lambda_1} \log f = \pd{^2}{\lambda_1 \partial \lambda_0} \log f = 0 \, .
\end{equation*}
%
So the off-diagonal elements of the Hessian are 0. Thus the diagonal elements are the eigenvalues, and if they are negative the Hessian is negative definite. We get
%
\begin{align*}
    \pd[2]{}{\lambda_0} \log f &= - \frac{k}{\lambda_0^2} - \sum_{i=1}^n (1-u_i) \frac{z_i^2 e^{\lambda_0 z_i}}{(e^{\lambda_0 z_i -1)^2}} \, , \quad \text{and} \\
    \pd[2]{^2}{\lambda_1} \log f &= - \frac{n-k}{\lambda_1^2} - \sum_{i=1}^n u_i \frac{z_i^2 e^{\lambda_1 z_i}}{(e^{\lambda_1 z_i -1)^2}} \, .
\end{align*}
%
Since $u_i$ are not all zero, the diagonal elements are $<0$. So we have proven that if we find a maximum it is the global maximum. The maximum likelihood estimators are found numerically in the following code.

<<p3_4, eval=FALSE>>=
@

The result is $\vect{\lambda}_{MLE} =$ (\Sexpr{nice(lambda_mle)}). This is very close to the result from the expectation maximization algorithm $\vect{\lambda}_{EM} =$ (\Sexpr{nice(lambda_em)}).

One advantage of maximizing the likelihood directly is that we are sure that we actually maximize the right thing; we maximize the likelihood and not the complete likelihood as in EM, where we also have to estimate values for $\vect{x}$ and $\vect{y}$. Another advantage is that we only have to maximize once, while with EM we have to maximize once for each iteration. Which in general can be computationally expensive. Furthermore, now that we have the Hessian, we can use it to find the amount of information in the data (if we can find the expectation of the second order partial derivatives). 

In some cases, however, the numerical calculations to solve the equation(s) corresponding to setting \eqref{eq:p3_score} to 0 are computationally expensive, and less robust than the EM algorithm. And in other cases it might not even be possible to find the likelihood for the observed data; integrating out the hidden variables might not be possible analytically.

